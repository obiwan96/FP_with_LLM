import optuna
from torch import nn
import torch.nn.functional as F
import torch
import pandas as pd
import numpy as np
from scipy.stats import skew, ttest_ind
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.metrics import f1_score
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import os
from functools import partial
import optuna.visualization as vis
from optuna.pruners import MedianPruner

# ---------- ëª¨ë¸ ì •ì˜ ----------
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3, temperature=1.0):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.temperature = nn.Parameter(torch.tensor(temperature))
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :]).squeeze(-1) / self.temperature

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3, temperature=1.0):
        super().__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.temperature = nn.Parameter(torch.tensor(temperature))
    def forward(self, x):
        out, _ = self.gru(x)
        #return self.fc(out[:, -1, :])
        out = out.mean(dim=1)
        return self.fc(out).squeeze(-1) / self.temperature

class Attention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.query = nn.Linear(dim, dim)
        self.key = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.scale = dim ** -0.5
    def forward(self, x):
        q, k, v = self.query(x), self.key(x), self.value(x)
        attn = torch.softmax((q @ k.transpose(-2, -1)) * self.scale, dim=-1)
        return attn @ v

class GRUWithAttention(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.3, temperature=1.0):
        super().__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.attn = Attention(hidden_size)
        self.fc = nn.Linear(hidden_size, 1)
        self.temperature = nn.Parameter(torch.tensor(temperature))
    def forward(self, x):
        out, _ = self.gru(x)
        attn_out = self.attn(out)
        return self.fc(attn_out[:, -1, :]).squeeze(-1) / self.temperature
    
class ConvGRU(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.4, temperature=1.0):
        super().__init__()
        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=5, padding=1)
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.temperature = nn.Parameter(torch.tensor(temperature))

    def forward(self, x):
        # x: [B, T, D] â†’ conv expects [B, D, T]
        x = x.permute(0, 2, 1)
        x = self.conv1(x).permute(0, 2, 1)
        out, _ = self.gru(x)
        out = out.mean(dim=1)
        return self.fc(out).squeeze(-1) / self.temperature

class TCNBlock(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):
        super().__init__()
        padding = (kernel_size - 1) * dilation
        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)
        self.relu = nn.ReLU()
        self.net = nn.Sequential(self.conv, self.relu)
    def forward(self, x):
        out = self.net(x)
        return out[:, :, :-self.conv.padding[0]]  # trim causal padding

class TCNModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, levels=3):
        super().__init__()
        layers = []
        for i in range(levels):
            in_ch = input_size if i == 0 else hidden_size
            layers.append(TCNBlock(in_ch, hidden_size, dilation=2**i))
        self.tcn = nn.Sequential(*layers)
        self.fc = nn.Linear(hidden_size, 2)
    def forward(self, x):
        x = x.transpose(1, 2)
        out = self.tcn(x)
        out = out.transpose(1, 2)
        return self.fc(out[:, -1, :])
    
def focal_loss_ce(logits, targets, alpha=0.9, gamma=1.0, reduction='mean'):
    #ce = F.binary_cross_entropy_with_logits(logits.squeeze(), targets.float(), reduction='none')
    ce = F.binary_cross_entropy_with_logits(logits.view(-1), targets.float().view(-1), reduction='none')

    pt = torch.exp(-ce)
    loss = alpha * ((1 - pt) ** gamma) * ce
    return loss.mean()

##########
## Dataset ê´€ë ¨

def get_cutoff_time_by_failure_ratio(failures, train_ratio=0.8):
    """
    failures ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° train_ratio ë¹„ìœ¨ë§Œí¼ trainì— í¬í•¨ë˜ë„ë¡ cutoff ì‹œì  ê³„ì‚°.
    """
    failures = pd.to_datetime(failures)
    failures_sorted = np.sort(failures)
    if len(failures_sorted) == 0:
        raise ValueError("âš ï¸ failures list is empty")

    cutoff_idx = int(np.ceil(len(failures_sorted) * train_ratio)) - 1
    cutoff_idx = max(0, min(cutoff_idx, len(failures_sorted) - 1))
    cutoff_time = failures_sorted[cutoff_idx]

    #print(f"[INFO] Cutoff time = {cutoff_time}")
    #print(f"[INFO] Train failures: {cutoff_idx + 1}/{len(failures_sorted)} "  f"({(cutoff_idx + 1)/len(failures_sorted):.1%})")
    return cutoff_time

def split_by_cutoff(X_seq, y_seq, ts_seq, cutoff_time, put_cutoff_to_train=True):
    """
    cutoff ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ X/y/timestamp split.
    """
    ts_seq = pd.to_datetime(ts_seq)
    if put_cutoff_to_train:
        train_mask = ts_seq <= cutoff_time
        test_mask = ts_seq > cutoff_time
    else:
        train_mask = ts_seq < cutoff_time
        test_mask = ts_seq >= cutoff_time

    X_train, X_test = X_seq[train_mask], X_seq[test_mask]
    y_train, y_test = y_seq[train_mask], y_seq[test_mask]

    #print(f"[INFO] Train samples: {len(X_train)} | Test samples: {len(X_test)}")
    #print(f"[INFO] Train positives: {np.sum(y_train==1)} | Test positives: {np.sum(y_test==1)}")
    return X_train, X_test, y_train, y_test


def event_cv_split(failures, n_folds=3):
    failures = sorted(failures)
    folds = []
    fold_size = max(1, len(failures) // n_folds)
    for i in range(n_folds):
        val = failures[i*fold_size:(i+1)*fold_size]
        train = [f for f in failures if f not in val]
        if val:
            folds.append((train, val))
    return folds

def make_dataset(X, y, window, horizon):
    xs, ys = [], []
    for i in range(len(X) - window - horizon):
        xs.append(X[i : i + window])           # ê³¼ê±° êµ¬ê°„
        future_window = y[i + window -1 : i + window + horizon-1]
        label = 1 if np.any(future_window > 0) else 0 
        ys.append(label)
    return np.array(xs), np.array(ys)

def make_soft_dataset(X, y, timestamps, window, horizon, smooth_window=3, mode="linear"):
    """
    window/horizon ê¸°ë°˜ soft label ë°ì´í„°ì…‹ ìƒì„± + ê° ì‹œí€€ìŠ¤ì˜ ëŒ€í‘œ timestamp ì €ì¥
    """
    xs, ys, ts_valid = [], [], []
    N = len(X)
    for i in range(N - window - horizon+1):
        x_window = X[i : i + window]
        if horizon > 0:
            current_y_window = y[i:i + window]
            if np.any(current_y_window > 0):
                continue  # if not detection, fault inside window â†’ skip

        future_window = y[i + window-1: i + window + horizon]
        label = 1 if np.any(future_window > 0) else 0

        if not label:
            future_y = y[i + window + horizon : i + window + horizon + smooth_window]
            if np.any(future_y > 0):
                dist = np.argmax(future_y > 0)
                if mode == "linear":
                    #label = max(0.0, 1.0 - dist / horizon)
                    label = max(0, 1 - dist / (smooth_window))
                elif mode == "exp":
                    label = np.exp(-dist / (smooth_window))
        xs.append(x_window)
        ys.append(label)
        ts_valid.append(timestamps[i + window + horizon - 1])

    return np.array(xs), np.array(ys), np.array(ts_valid)

def smooth_labels(y, eps=0.05):
    """
    Label smoothing for imbalanced soft targets.
    y: numpy array or torch tensor (0~1)
    eps: smoothing factor (default 0.05 â†’ 5% ë¶€ë“œëŸ½ê²Œ)
    """
    y = np.asarray(y, dtype=np.float32)
    y_smooth = y * (1 - eps) + eps * 0.5
    return y_smooth

# feature ë¶„ì„/ í•™ìŠµ ê´€ë ¨
def train_and_eval(model, train_loader, val_loader, optimizer, criterion, device, scheduler=None):
    model.train()
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        out = model(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer.step()
    if scheduler:
        scheduler.step(loss.item())

    # --- Validation ---
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            out = model(xb)
            preds.extend(torch.sigmoid(out).cpu().numpy())
            trues.extend(yb.numpy())
    preds, trues = np.array(preds), np.array(trues)
    ths = np.linspace(0.1, 0.9, 9)
    f1s = [f1_score((trues > 0.5).astype(int), (preds > t).astype(int)) for t in ths]
    best_idx = np.argmax(f1s)
    best_th = ths[best_idx]
    f1_best = f1s[best_idx]
    roc = roc_auc_score((trues > 0.5).astype(int), preds)
    pr = average_precision_score((trues > 0.5).astype(int), preds)
    return f1_best, roc, pr, best_th

# âœ… Optuna objective
def objective(trial, X_raw, y_raw, timestamps, failures, device, model_name = None):
    hidden_size = trial.suggest_int("hidden_size", 32, 256, step=32)
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    lr = trial.suggest_loguniform("lr", 1e-5, 1e-3)
    alpha = trial.suggest_float("alpha", 0.6, 1.0)
    gamma = trial.suggest_float("gamma", 0.5, 3.0)
    temperature = trial.suggest_float("temperature", 0.5, 3.0)
    window = trial.suggest_int("window", 5,15, step=5)
    horizon = trial.suggest_categorical("horizon", [0,1,2,3, 5])
    if model_name is None:
        model_name = trial.suggest_categorical("model_name", ["LSTM", "GRU", "GRU_Att", "CNV_GRU"])

    X_seq, y_seq, ts_seq = make_soft_dataset(X_raw, y_raw, timestamps, window, horizon, mode="linear")

    # --- Event-based 2-fold CV ---
    train_ratio_list = [0.3, 0.7]
    fold_scores = []
    for train_ratio in train_ratio_list:
        cutoff_time = get_cutoff_time_by_failure_ratio(failures, train_ratio)
        if train_ratio == 0.3:
            X_val, X_tr, y_val, y_tr = split_by_cutoff(X_seq, y_seq, ts_seq, cutoff_time, put_cutoff_to_train=False)
        else:
            X_tr, X_val, y_tr, y_val = split_by_cutoff(X_seq, y_seq, ts_seq, cutoff_time)
        #print(np.unique(y_val, return_counts=True))
        if model_name == "LSTM":
            model = LSTMModel(input_size=X_tr.shape[2],
                              hidden_size=hidden_size,
                              dropout=dropout,
                              temperature=temperature).to(device)
        elif model_name == "GRU":
            model = GRUModel(input_size=X_tr.shape[2],
                             hidden_size=hidden_size,
                             dropout=dropout,
                             temperature=temperature).to(device)
        elif model_name == "GRU_Att":
            model = GRUWithAttention(input_size=X_tr.shape[2],
                                     hidden_size=hidden_size,
                                     dropout=dropout,
                                     temperature=temperature).to(device)
        elif model_name == "CNV_GRU":
            model = ConvGRU(input_size=X_tr.shape[2],
                            hidden_size=hidden_size,
                            dropout=dropout,
                            temperature=temperature).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)

        criterion = lambda logits, targets: focal_loss_ce(logits, targets, alpha=alpha, gamma=gamma)
        train_loader = DataLoader(TensorDataset(torch.tensor(X_tr, dtype=torch.float32),
                                                torch.tensor(y_tr, dtype=torch.float32)), batch_size=64, shuffle=False)
        val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                                              torch.tensor(y_val, dtype=torch.float32)), batch_size=64, shuffle=False)
        best_f1 = 0
        for epoch in range(1, 100):
            f1, auc, pr, _ = train_and_eval(model, train_loader, val_loader,
                                            optimizer, criterion, device, scheduler)
            best_f1 = max(best_f1, f1)
        fold_scores.append(best_f1)

    score = np.mean(fold_scores)
    trial.report(score, step=1)
    return score

def study_optuna(X_raw, y_raw, timestamps, failures, device, timeout=None, model_name=None):
    # TPE + Media pruner
    sampler = optuna.samplers.TPESampler(seed=42)
    pruner=MedianPruner(
        n_startup_trials=10,     # ì´ˆë°˜ 10ê°œ trialì€ pruningí•˜ì§€ ì•ŠìŒ
        n_warmup_steps=5,       # ìµœì†Œ 5 step ì´í›„ë¶€í„° pruning ê³ ë ¤
        interval_steps=1
    )
    study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner)
    objective_with_data = partial(
        objective,
        X_raw=X_raw,
        y_raw=y_raw,
        timestamps=timestamps,
        failures=failures,
        device=device,
        model_name=model_name
    )

    study.optimize(objective_with_data, timeout=timeout)
    print("âœ… Best Params:", study.best_params)
    print("âœ… Best Composite Score:", study.best_value)
    if len(study.trials) > 1:
        fig1 = vis.plot_optimization_history(study)
        fig1.write_html("tmp/optuna/optuna_optimization_history.html")

        fig2 = vis.plot_param_importances(study)
        fig2.write_html("tmp/optuna/optuna_param_importances.html")

        fig3 = vis.plot_parallel_coordinate(study)
        fig3.write_html("tmp/optuna/optuna_parallel_coordinate.html")
    return study

def analyze_features_cli(X, feature_names=None, output_dir="tmp/", skew_threshold=10):
    """
    ê° featureë³„ í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ì‚°, ì™œë„(skewness)ë¥¼ ê³„ì‚°í•˜ê³ 
    log ë³€í™˜ì´ í•„ìš”í•œ featureë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    CLI í™˜ê²½ì—ì„œë„ ì‘ë™í•˜ë„ë¡ /tmp í´ë”ì— íˆìŠ¤í† ê·¸ë¨ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

    Parameters
    ----------
    X : np.ndarray or pd.DataFrame
        ì…ë ¥ ë°ì´í„° (shape: [samples, features])
    feature_names : list or None
        feature ì´ë¦„. Noneì´ë©´ f0, f1, ... ìë™ ìƒì„±
    output_dir : str
        ê·¸ë˜í”„ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: /tmp)
    skew_threshold : float
        ì ˆëŒ“ê°’ì´ ì´ ê°’ì„ ë„˜ëŠ” ê²½ìš° log ë³€í™˜ ì¶”ì²œ
    """
    # DataFrame ë³€í™˜
    X=X.fillna(0)
    if isinstance(X, np.ndarray):
        if feature_names is None:
            feature_names = [f"f{i}" for i in range(X.shape[1])]
        df = pd.DataFrame(X, columns=feature_names)
    else:
        df = X.copy()
        feature_names = df.columns

    # í†µê³„ëŸ‰ ê³„ì‚°
    stats = pd.DataFrame({
        "mean": df.mean(),
        "std": df.std(),
        "var": df.var(),
        "skewness": df.apply(skew)
    })
    
    stats["recommend_log"] = stats["skewness"].abs() > skew_threshold

    print("\nğŸ“Š Feature Statistics Summary\n")
    print(stats.round(4))
    print("\nğŸ’¡ Log transform recommended for these features:")
    print(stats[stats["recommend_log"]].index.tolist())

    # ì‹œê°í™” ì €ì¥
    print(f"\nğŸ“ Saving feature histograms to {output_dir}/ ...")
    for col in feature_names:
        plt.figure(figsize=(5, 3))
        plt.hist(df[col].dropna(), bins=40, color="steelblue", alpha=0.7)
        plt.title(f"{col}\nmean={df[col].mean():.2f}, skew={skew(df[col]):.2f}")
        plt.xlabel(col)
        plt.ylabel("Frequency")
        plt.tight_layout()

        # íŒŒì¼ ê²½ë¡œ
        save_path = os.path.join(output_dir, f"{col}_hist.png")
        plt.savefig(save_path)
        plt.close()

    print("âœ… All histograms saved successfully.\n")

    return stats[stats["recommend_log"]].index.tolist()

def analyze_feature_shift(xs, ys, feature_names):
    """
    ê¸°ëŠ¥:
      horizon ì´í›„ì— abnormal(=1)ì´ ë°œìƒí•œ ìœˆë„ìš° vs ê·¸ë ‡ì§€ ì•Šì€ ìœˆë„ìš°ì˜
      featureë³„ í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ì‚° ë¹„êµí‘œ ì¶œë ¥
    """
    xs = np.array(xs)  # shape [N, window, D]
    ys = np.array(ys)  # shape [N,]

    # ìœˆë„ìš° ë‚´ í‰ê· ê°’ (ì‹œê°„ì¶• í‰ê· )
    X_window_mean = xs.mean(axis=1)  # shape [N, D]

    # normal / abnormal ê·¸ë£¹ ë¶„ë¦¬
    X_normal = X_window_mean[ys == 0]
    X_abnormal = X_window_mean[ys == 1]

    # í†µê³„ ìš”ì•½í‘œ ìƒì„±
    stats_df = pd.DataFrame({
        "normal_mean": X_normal.mean(axis=0),
        "abnormal_mean": X_abnormal.mean(axis=0),
        "normal_std": X_normal.std(axis=0),
        "abnormal_std": X_abnormal.std(axis=0),
        "mean_diff": X_abnormal.mean(axis=0) - X_normal.mean(axis=0),
        "p_value": ttest_ind(X_abnormal, X_normal, equal_var=False)[1]
    }, index=feature_names)
    
    print("\nğŸ“Š Feature Statistics Comparison (normal vs abnormal)\n")
    print(stats_df)

    # P-value ê°€ ì‘ì€ feature ìƒìœ„ 3ê°œ í‘œì‹œ
    print("\nğŸ”¥ Top features with smallest P-value:")
    print(stats_df["p_value"].abs().sort_values(ascending=True).head(3))
    return stats_df
